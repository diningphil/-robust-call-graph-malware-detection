{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "from data.util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['final_run2',\n",
       " 'MODEL_SELECTION',\n",
       " 'final_run3',\n",
       " 'final_run1',\n",
       " 'outer_results.json']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_folder_cgmm = '/raid/errica/CIML_CNR_RESULTS/CGMM_CG_ClassifierCGMMTask/MODEL_ASSESSMENT/OUTER_FOLD_1/'\n",
    "os.listdir(exp_folder_cgmm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layer_config': {'embeddings_folder': '/raid/errica/CIML_CNR_RESULTS/DEGREE/CGMM_EMBEDDINGS', 'checkpoint': True, 'batch_size': 64, 'shuffle': False, 'previous_layers_to_use': '1', 'concatenate_on_axis': 1, 'max_layers': 20, 'layers': 20, 'C': 20, 'A': 1, 'predictor': 'model.predictor.node_predictor.UnsupervisedProbabilisticNodeReadout', 'emission': 'model.distribution.emission.IsotropicGaussian', 'unibigram': True, 'aggregation': 'sum', 'infer_with_posterior': False, 'epochs': 10, 'wrapper': 'training.engine.IncrementalTrainingEngine', 'loss': 'training.callback.loss.CGMMLoss', 'optimizer': 'training.callback.optimizer.CGMMOptimizer', 'scorer': {'class_name': 'training.callback.score.MultiScore', 'args': {'main_scorer': 'training.callback.score.CGMMCompleteLikelihoodScore', 'true_likelihood': 'training.callback.score.CGMMTrueLikelihoodScore'}}, 'plotter': 'training.callback.plotter.Plotter', 'arbitrary_function_config': {'shuffle': True, 'batch_size': 128, 'checkpoint': True, 'log_every': 1, 'num_dataloader_workers': 0, 'pin_memory': False, 'device': 'cpu', 'epochs': 500, 'hidden_units': 128, 'optimizer': {'class_name': 'training.callback.optimizer.Optimizer', 'args': {'optimizer_class_name': 'torch.optim.Adam', 'lr': 0.0001, 'weight_decay': 0.0005}}, 'loss': 'training.callback.loss.MulticlassClassificationLoss', 'scorer': 'training.callback.score.MulticlassAccuracyScore', 'predictor': 'model.predictor.graph_predictor.SimpleMLP', 'wrapper': 'training.engine.TrainingEngine', 'early_stopper': {'class_name': 'training.callback.early_stopping.PatienceEarlyStopper', 'args': {'patience': 100, 'monitor': 'validation_Multiclass Accuracy', 'mode': 'max', 'checkpoint': True}}, 'plotter': 'training.callback.plotter.Plotter'}}, 'dataset': 'CG', 'dataset_getter': 'data.provider.IncrementalDataProvider', 'dataset_class': 'data.dataset.CNRMalwareDataset', 'data_root': 'DATA_NOFEATS/', 'model': 'model.dgn.cgmm.CGMM', 'device': 'cpu', 'num_dataloader_workers': 0, 'pin_memory': False, 'experiment': 'experiment.cgmm_classifier_task.ClassifierCGMMTask', 'higher_results_are_better': True, 'log_every': 1, 'n_tasks': None, 'n_rehearsal_patterns_per_task': None}\n"
     ]
    }
   ],
   "source": [
    "with open(osp.join(exp_folder_cgmm, 'MODEL_SELECTION', 'winner_config.json'), 'r') as f:\n",
    "    d = json.load(f)\n",
    "    print(d['config'])\n",
    "# best classifier config chose CGMM configuration:\n",
    "C = 20\n",
    "max_layers = 20\n",
    "epochs = 10\n",
    "unibigram = True\n",
    "aggregation = 'sum'\n",
    "infer_with_posterior = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layer_config': {'embeddings_folder': '/raid/errica/CIML_CNR_RESULTS/DEGREE/CGMM_EMBEDDINGS', 'checkpoint': True, 'batch_size': 32, 'shuffle': False, 'previous_layers_to_use': '1', 'concatenate_on_axis': 1, 'max_layers': 20, 'C': 20, 'A': 1, 'predictor': 'model.predictor.node_predictor.UnsupervisedProbabilisticNodeReadout', 'emission': 'model.distribution.emission.IsotropicGaussian', 'unibigram': True, 'aggregation': 'sum', 'infer_with_posterior': False, 'epochs': 10, 'wrapper': 'training.engine.IncrementalTrainingEngine', 'loss': 'training.callback.loss.CGMMLoss', 'optimizer': 'training.callback.optimizer.CGMMOptimizer', 'scorer': {'class_name': 'training.callback.score.MultiScore', 'args': {'main_scorer': 'training.callback.score.CGMMCompleteLikelihoodScore', 'true_likelihood': 'training.callback.score.CGMMTrueLikelihoodScore'}}, 'plotter': 'training.callback.plotter.Plotter'}, 'dataset': 'CG', 'dataset_getter': 'data.provider.IncrementalDataProvider', 'dataset_class': 'data.dataset.CNRMalwareDataset', 'data_root': 'DATA_NOFEATS/', 'model': 'model.dgn.cgmm.CGMM', 'device': 'cuda', 'num_dataloader_workers': 2, 'pin_memory': True, 'experiment': 'experiment.cgmm_embedding_task.EmbeddingCGMMTask', 'higher_results_are_better': True, 'log_every': 1, 'n_tasks': None, 'n_rehearsal_patterns_per_task': None}\n"
     ]
    }
   ],
   "source": [
    "# config id 2 corresponds to the required configuration\n",
    "config_id = 4\n",
    "exp_folder = f'/raid/errica/CIML_CNR_RESULTS/CGMM_CG_EmbeddingCGMMTask/MODEL_ASSESSMENT/OUTER_FOLD_1/MODEL_SELECTION/config_{config_id}'\n",
    "os.listdir(exp_folder)\n",
    "with open(osp.join(exp_folder, 'config_results.json'), 'r') as f:\n",
    "    model_config = json.load(f)['config']\n",
    "    print(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiment.cgmm_embedding_task import *\n",
    "from experiment.util import *\n",
    "\n",
    "checkpoint_folder = f'/raid/errica/CIML_CNR_RESULTS/CGMM_CG_EmbeddingCGMMTask/MODEL_ASSESSMENT/OUTER_FOLD_1/MODEL_SELECTION/config_{config_id}/INNER_FOLD_1/'\n",
    "exp_folder = f'/raid/errica/CIML_CNR_RESULTS/INFERENCE_TEST/'\n",
    "embeddings_folder = f'/raid/errica/CIML_CNR_RESULTS/INFERENCE_TEST/CGMM_EMBEDDINGS/'\n",
    "model_config['embeddings_folder'] = embeddings_folder\n",
    "\n",
    "model_config['device'] = 'cuda'\n",
    "model_config['num_dataloader_workers'] = 2\n",
    "model_config['pin_memory'] = 'True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['last_checkpoint.pth', 'tensorboard']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(osp.join(checkpoint_folder, 'layer_1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "outer_folds = 1\n",
    "inner_folds = 1\n",
    "splits_folder = 'SPLITS'\n",
    "#splits_filepath = 'SPLITS/CG_OBF_TEST/CG_OBF_TEST_outer1_inner1.splits'\n",
    "splits_filepath = 'SPLITS/CG/CG_outer1_inner1.splits'\n",
    "experiment = EmbeddingCGMMTask(model_config, exp_folder)\n",
    "logger = None\n",
    "# Create the dataset provider\n",
    "dataset_getter_class = s2c(model_config['dataset_getter'])\n",
    "dataset_getter = dataset_getter_class(model_config['data_root'],\n",
    "                                      splits_folder,\n",
    "                                      splits_filepath,\n",
    "                                      s2c(model_config['dataset_class']),\n",
    "                                      model_config['dataset'],\n",
    "                                      outer_folds,\n",
    "                                      inner_folds,\n",
    "                                      model_config['num_dataloader_workers'],\n",
    "                                      model_config['pin_memory'])\n",
    "# dataset_getter = dataset_getter_class('DATA_NOFEATS/',\n",
    "#                                       splits_folder,\n",
    "#                                       splits_filepath,\n",
    "#                                       s2c(model_config['dataset_class']),\n",
    "#                                       'CG_OBF_TEST',\n",
    "#                                       outer_folds,\n",
    "#                                       inner_folds,\n",
    "#                                       model_config['num_dataloader_workers'],\n",
    "#                                       model_config['pin_memory'])\n",
    "\n",
    "dataset_getter.set_outer_k(0)\n",
    "dataset_getter.set_inner_k(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored outputs\n",
      "Stored outputs\n",
      "Stored outputs\n",
      "Stored outputs\n",
      "Stored outputs\n",
      "Stored outputs\n",
      "Stored outputs\n",
      "Stored outputs\n",
      "Stored outputs\n",
      "Stored outputs\n",
      "Stored outputs\n",
      "Stored outputs\n",
      "Stored outputs\n",
      "Stored outputs\n",
      "Stored outputs\n",
      "Stored outputs\n",
      "Stored outputs\n",
      "Stored outputs\n",
      "Stored outputs\n",
      "Stored outputs\n"
     ]
    }
   ],
   "source": [
    "batch_size = model_config['layer_config']['batch_size']\n",
    "shuffle = False\n",
    "\n",
    "# Instantiate the Dataset\n",
    "dim_node_features = dataset_getter.get_dim_node_features()\n",
    "dim_edge_features = dataset_getter.get_dim_edge_features()\n",
    "dim_target = dataset_getter.get_dim_target()\n",
    "\n",
    "layers = []\n",
    "l_prec = model_config['layer_config']['previous_layers_to_use'].split(',')\n",
    "concatenate_axis = model_config['layer_config']['concatenate_on_axis']\n",
    "max_layers = model_config['layer_config']['max_layers']\n",
    "assert concatenate_axis > 0, 'You cannot concat on the first axis for design reasons.'\n",
    "\n",
    "\n",
    "dict_per_layer = []\n",
    "stop = False\n",
    "depth = 1\n",
    "while not stop and depth <= max_layers:\n",
    "    # Change exp path to allow Stop & Resume\n",
    "    experiment.exp_path = os.path.join(experiment.root_exp_path, f'layer_{depth}')\n",
    "\n",
    "    test_folder = os.path.join(experiment.root_exp_path, 'outputs', 'test')\n",
    "    #if os.path.exists(os.path.join(test_folder, f'graph_output_{depth}.pt')):\n",
    "    #        print(\"skip layer\", depth)\n",
    "    #        depth += 1\n",
    "    #        continue\n",
    "\n",
    "\n",
    "    # load output will concatenate in reverse order (WHY?? TODOR REFACTOR)\n",
    "    prev_outputs_to_consider = [(depth - int(x)) for x in l_prec if (depth - int(x)) > 0]\n",
    "\n",
    "    train_out = experiment._create_extra_dataset(prev_outputs_to_consider, mode='train', depth=depth)\n",
    "    val_out = experiment._create_extra_dataset(prev_outputs_to_consider, mode='validation', depth=depth)\n",
    "    train_loader = dataset_getter.get_inner_train(batch_size=batch_size, shuffle=False, extra=train_out)\n",
    "    val_loader = dataset_getter.get_inner_val(batch_size=batch_size, shuffle=False, extra=val_out)\n",
    "\n",
    "\n",
    "    # ==== # WARNING: WE ARE JUST PRECOMPUTING OUTER_TEST EMBEDDINGS FOR SUBSEQUENT CLASSIFIERS\n",
    "    # WE ARE NOT TRAINING ON TEST (EVEN THOUGH UNSUPERVISED)\n",
    "    # ==== #\n",
    "\n",
    "    test_out = experiment._create_extra_dataset(prev_outputs_to_consider, mode='test', depth=depth)\n",
    "    test_loader = dataset_getter.get_outer_test(batch_size=batch_size, shuffle=False, extra=test_out)\n",
    "\n",
    "\n",
    "    # THE OBFUSCATED TEST SET HAS BEEN PUT INTO \"TRAIN\" SET OF THE FAKE DATASET USED TO PERFORM INFERENCE ON THE NEW OBFUSCATED SAMPLES\n",
    "    #print(len(train_loader.dataset))\n",
    "    #print(len(val_loader.dataset))\n",
    "    #print(len(test_loader.dataset))\n",
    "\n",
    "    # ==== #\n",
    "\n",
    "    # Instantiate the Model\n",
    "    new_layer = experiment.create_incremental_model(dim_node_features, dim_edge_features, dim_target, depth, prev_outputs_to_consider)\n",
    "    ckpt_filename = osp.join(checkpoint_folder, f'layer_{depth}', 'last_checkpoint.pth')\n",
    "    model_state = torch.load(ckpt_filename)['model_state']\n",
    "    new_layer.load_state_dict(model_state)\n",
    "\n",
    "    # Instantiate the wrapper (it handles the training loop and the inference phase by abstracting the specifics)\n",
    "    incremental_training_wrapper = experiment.create_incremental_wrapper(new_layer)\n",
    "\n",
    "    train_loss, _, train_out = incremental_training_wrapper.infer(loader=train_loader, set=incremental_training_wrapper.TRAINING, return_node_embeddings=True)\n",
    "    val_loss, _, val_out = incremental_training_wrapper.infer(loader=val_loader, set=incremental_training_wrapper.VALIDATION, return_node_embeddings=True)\n",
    "    test_loss, _, test_out = incremental_training_wrapper.infer(loader=test_loader, set=incremental_training_wrapper.TEST, return_node_embeddings=True)    \n",
    "    \n",
    "    for loader, out, mode in [(train_loader, train_out, 'train'), (val_loader, val_out, 'validation'), (test_loader, test_out, 'test')]:\n",
    "        v_out, e_out, g_out, vo_out, eo_out, go_out = out\n",
    "\n",
    "        # Reorder outputs, which are produced in shuffled order, to the original arrangement of the dataset.\n",
    "        v_out, e_out, g_out, vo_out, eo_out, go_out = experiment._reorder_shuffled_objects(v_out, e_out, g_out, vo_out, eo_out, go_out, loader)\n",
    "\n",
    "        # Store outputs\n",
    "        experiment._store_outputs(mode, depth, v_out, e_out, g_out, vo_out, eo_out, go_out)\n",
    "\n",
    "    print('Stored outputs')\n",
    "\n",
    "    depth += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded all previous outputs\n"
     ]
    }
   ],
   "source": [
    "# NOW LOAD ALL EMBEDDINGS AND STORE THE EMBEDDINGS DATASET ON a torch file.\n",
    "\n",
    "# Consider all previous layers now, i.e. gather all the embeddings\n",
    "prev_outputs_to_consider = [l for l in range(1, max_layers+1)]\n",
    "prev_outputs_to_consider.reverse()  # load output will concatenate in reverse order (WHY?? TODOR REFACTOR)\n",
    "\n",
    "# Retrieve only the graph embeddings to save memory.\n",
    "# In CGMM classfication task (see other experiment file), I will ignore the outer val and reuse the inner val as validation, as I cannot use the splitter.\n",
    "#train_out = experiment._create_extra_dataset(prev_outputs_to_consider, mode='train', depth=depth, only_g=True)\n",
    "#val_out = experiment._create_extra_dataset(prev_outputs_to_consider, mode='validation', depth=depth, only_g=True)\n",
    "test_out = experiment._create_extra_dataset(prev_outputs_to_consider, mode='test', depth=depth, only_g=True)\n",
    "\n",
    "print('Loaded all previous outputs')\n",
    "\n",
    "# Necessary info to give a unique name to the dataset (some hyper-params like epochs are assumed to be fixed)\n",
    "embeddings_folder = f'/raid/errica/CIML_CNR_RESULTS/INFERENCE_TEST/CGMM_EMBEDDINGS/'\n",
    "max_layers = model_config['layer_config']['max_layers']\n",
    "unibigram = model_config['layer_config']['unibigram']\n",
    "C = model_config['layer_config']['C']\n",
    "CA = model_config['layer_config']['CA'] if 'CA' in model_config['layer_config'] else None\n",
    "aggregation = model_config['layer_config']['aggregation']\n",
    "infer_with_posterior = model_config['layer_config']['infer_with_posterior']\n",
    "outer_k = dataset_getter.outer_k\n",
    "inner_k = dataset_getter.inner_k\n",
    "# ===="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving to /raid/errica/CIML_CNR_RESULTS/DEGREE/INFERENCE_TEST/CGMM_EMBEDDINGS/CG/20_False_20_None_sum_False_1_1_train.torch\n",
      "saving to /raid/errica/CIML_CNR_RESULTS/DEGREE/INFERENCE_TEST/CGMM_EMBEDDINGS/CG/20_True_20_None_sum_False_1_1_train.torch\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(os.path.join(embeddings_folder, dataset_getter.dataset_name)):\n",
    "    os.makedirs(os.path.join(embeddings_folder, dataset_getter.dataset_name))\n",
    "\n",
    "for unib in [False, True]:\n",
    "    unigram_dim = C+CA if CA is not None else C\n",
    "    base_path = os.path.join(embeddings_folder, dataset_getter.dataset_name, f'{max_layers}_{unib}_{C}_{CA}_{aggregation}_{infer_with_posterior}_{outer_k+1}_{inner_k+1}')\n",
    "    print(f'saving to {base_path}_train.torch')\n",
    "    #train_out_emb = torch.cat([d.g_outs if unib else d.g_outs[:, :, :unigram_dim] for d in train_out], dim=0)\n",
    "    #torch.save(train_out_emb, base_path + '_train.torch')\n",
    "    #val_out_emb = torch.cat([d.g_outs if unib else d.g_outs[:, :, :unigram_dim] for d in val_out], dim=0)\n",
    "    #torch.save(val_out_emb, base_path + '_val.torch')\n",
    "    test_out_emb = torch.cat([d.g_outs if unib else d.g_outs[:, :, :unigram_dim] for d in test_out], dim=0)\n",
    "    torch.save(test_out_emb, base_path + '_test.torch')\n",
    "\n",
    "# CLEAR OUTPUTS\n",
    "#for mode in ['train', 'validation', 'test']:\n",
    "#    shutil.rmtree(os.path.join(experiment.output_folder, mode), ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
